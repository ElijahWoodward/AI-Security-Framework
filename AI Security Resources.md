| Resource                                                                | Description                                                                                                                                                                                                                                                                                              | URL                                                                                                           |
|-------------------------------------------------------------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------------|
| **MITRE ATLAS™**                                                        | A comprehensive knowledge base of adversary tactics and techniques for AI systems, providing a structured approach to understanding and mitigating threats.                                                                                                                                              | [https://atlas.mitre.org/](https://atlas.mitre.org/)                                                         |
| **Offensive Security with Large Language Models (1)**                   | Discusses the application of LLMs in offensive security, focusing on automatic vulnerability detection and fuzzing techniques. Highlights how models like ChatGPT are evolving to interpret deeper logic and flow of code and binaries.                                                                   | [https://blog.xint.io/offensive-security-with-large-language-models-1-f5d70b0ad51d](https://blog.xint.io/offensive-security-with-large-language-models-1-f5d70b0ad51d)                   |
| **Breaking the Rules: Jailbreak Attacks on Large Language Models**      | An exploration of jailbreak attacks (prompt injection attacks) that bypass security measures in LLMs to produce outputs violating their intended purpose or safety guidelines.                                                                                                                            | [https://www.fuzzylabs.ai/blog-post/jailbreak-attacks-on-large-language-models](https://www.fuzzylabs.ai/blog-post/jailbreak-attacks-on-large-language-models)                         |
| **Compare 20 LLM Security Tools & Open-Source Frameworks [2025]**       | A detailed benchmark to help choose the best LLM security tool that can deliver comprehensive protection for large language models.                                                                                                                                                                       | [https://research.aimultiple.com/llm-security-tools/](https://research.aimultiple.com/llm-security-tools/)                                       |
| **How to Jailbreak LLMs One Step at a Time: Top Techniques and Strategies** | An overview of notable attacks on LLMs (prompt injection, data poisoning, denial-of-service) focusing on exploiting LLMs into generating undesirable or harmful outputs.                                                                                                                                | [https://www.confident-ai.com/blog/how-to-jailbreak-llms-one-step-at-a-time](https://www.confident-ai.com/blog/how-to-jailbreak-llms-one-step-at-a-time)                              |
| **How to build an offensive AI security agent**                         | Covers an offensive security perspective of an external attacker with no insight into an organization's assets, focusing on black box testing to find vulnerabilities in applications.                                                                                                                     | [https://www.anshumanbhartiya.com/posts/hackagent](https://www.anshumanbhartiya.com/posts/hackagent)                                             |
| **How to Protect LLMs from Jailbreaking Attacks**                       | Booz Allen explores novel cybersecurity approaches to prevent jailbreaking attacks against AI models, emphasizing continuous testing and improvements in AI defenses.                                                                                                                                     | [https://www.boozallen.com/insights/ai-research/how-to-protect-llms-from-jailbreaking-attacks.html](https://www.boozallen.com/insights/ai-research/how-to-protect-llms-from-jailbreaking-attacks.html) |
| **LLM Security—Risks, Vulnerabilities, and Mitigation Measures**        | Discusses the main security weaknesses of LLM applications, outlines prevalent attack methods, and provides strategies to mitigate such risks, emphasizing the importance of best practices in LLM security.                                                                                               | [https://nexla.com/ai-infrastructure/llm-security/](https://nexla.com/ai-infrastructure/llm-security/)                                             |
| **Towards Understanding Jailbreak Attacks in LLMs**                     | A research paper exploring how harmful and harmless prompts behave in the LLM representation space, investigating intrinsic properties of successful jailbreak attacks.                                                                                                                                  | [https://arxiv.org/abs/2406.10794](https://arxiv.org/abs/2406.10794)                                                                               |
| **Deep Dive into LLM Security: Leading Security Frameworks**            | An overview of prominent frameworks and guidelines relevant to LLM security, including the OWASP Top 10 for LLMs and MITRE ATLAS.                                                                                                                                                                         | [https://www.byreference.net/deep-dive-into-llm-security-leading-security-frameworks/](https://www.byreference.net/deep-dive-into-llm-security-leading-security-frameworks/)           |
| **OWASP Top 10 for Large Language Model Applications**                  | A list highlighting the most critical vulnerabilities often seen in LLM applications, emphasizing impact, exploitability, and real-world prevalence.                                                                                                                                                      | [https://owasp.org/www-project-top-10-for-large-language-model-applications/](https://owasp.org/www-project-top-10-for-large-language-model-applications/)                             |
| **Jailbreaking Black Box Large Language Models in Twenty Queries**      | Introduces Prompt Automatic Iterative Refinement (PAIR)—an algorithm that generates semantic jailbreaks with only black-box access to an LLM, inspired by social engineering attacks.                                                                                                                     | [https://jailbreaking-llms.github.io/](https://jailbreaking-llms.github.io/)                                                                     |
| **Artificial Intelligence as the New Hacker: Developing Agents for Offensive Security** | Explores integrating AI into offensive cybersecurity through developing an autonomous AI agent designed to simulate and execute cyberattacks.                                                                                                                                                              | [https://arxiv.org/abs/2406.07561](https://arxiv.org/abs/2406.07561)                                                                               |
| **AutoAttacker: A Large Language Model Guided System to Implement Automatic Cyber-attacks** | Focuses on leveraging LLMs to simulate post-breach, hands-on-keyboard attacks under various techniques and environments.                                                                                                                                                                                   | [https://arxiv.org/abs/2403.01038](https://arxiv.org/abs/2403.01038)                                                                               |
| **greshake/llm-security**                                              | A GitHub repo presenting a new class of vulnerabilities and impacts from "indirect prompt injection" in language models integrated with applications.                                                                                                                                                     | [https://github.com/greshake/llm-security](https://github.com/greshake/llm-security)                                                               |
| **Tree of Attacks: Jailbreaking Black-Box LLMs Automatically**          | Introduces Tree of Attacks with Pruning (TAP), an automated method for generating jailbreaks with only black-box access to the target LLM.                                                                                                                                                                | [https://arxiv.org/abs/2312.02119](https://arxiv.org/abs/2312.02119)                                                                               |
| **corca-ai/awesome-llm-security**                                      | A curated list of tools, documents, and projects related to LLM security, encompassing various attack methods and defense strategies.                                                                                                                                                                     | [https://github.com/corca-ai/awesome-llm-security](https://github.com/corca-ai/awesome-llm-security)                                               |
| **MITRE's Sensible Regulatory Framework for AI Security**              | Provides high-level guidance on approaching AI security from a policy perspective, referencing the ATLAS Matrix for specific threats and mitigations.                                                                                                                                                     | [https://www.paloaltonetworks.com/cyberpedia/mitre-sensible-regulatory-framework-atlas-matrix](https://www.paloaltonetworks.com/cyberpedia/mitre-sensible-regulatory-framework-atlas-matrix) |
| **tmylla/Awesome-LLM4Cybersecurity**                                   | A systematic literature review of LLM applications in cybersecurity, including threat intelligence and vulnerability detection.                                                                                                                                                                            | [https://github.com/tmylla/Awesome-LLM4Cybersecurity](https://github.com/tmylla/Awesome-LLM4Cybersecurity)                                         |
| **llm-attacks**                                                        | Focuses on universal and transferable attacks on aligned language models, providing insights into potential vulnerabilities.                                                                                                                                                                               | [https://github.com/llm-attacks](https://github.com/llm-attacks)                                                                                   |
| **mitre/advmlthreatmatrix**                                           | The Adversarial ML Threat Matrix positions machine learning attacks in an ATT&CK-style framework to help security analysts understand emerging threats.                                                                                                                                                    | [https://github.com/mitre/advmlthreatmatrix](https://github.com/mitre/advmlthreatmatrix)                                                           |
| **ydyjya/Awesome-LLM-Safety**                                         | A curated list of safety-related papers, articles, and resources focused on Large Language Models, aiming to provide insights into safety implications and challenges.                                                                                                                                     | [https://github.com/ydyjya/Awesome-LLM-Safety](https://github.com/ydyjya/Awesome-LLM-Safety)                                                       |
| **Security Attacks on LLM-based Code Completion Tools**                | A research paper exploring vulnerabilities in LLM-based code completion tools, including jailbreaking and training data extraction attacks.                                                                                                                                                               | [https://arxiv.org/html/2408.11006v4](https://arxiv.org/html/2408.11006v4)                                                                         |
| **LLM-Powered Code Vulnerability Repair with Reinforcement Learning and Semantic Reward** | Discusses identifying and generating fixes for code vulnerabilities using LLMs, enhanced by reinforcement learning and semantic rewards.                                                                                                                                                                  | [https://arxiv.org/abs/2401.03374](https://arxiv.org/abs/2401.03374)                                                                               |
| **Unveiling A Hidden Risk: Exposing Educational but Malicious Repositories in GitHub** | Investigates the presence of malicious code in educational repositories on GitHub, highlighting potential security risks.                                                                                                                                                                                 | [https://arxiv.org/abs/2403.04419](https://arxiv.org/abs/2403.04419)                                                                               |
| **Anatomy of an AI ATTACK: MITRE ATLAS - YouTube**                      | A video presentation providing an overview of the MITRE ATLAS framework and its application in understanding AI system vulnerabilities.                                                                                                                                                                    | [https://www.youtube.com/watch?v=QhoG74PDFyc](https://www.youtube.com/watch?v=QhoG74PDFyc)                                                         |
